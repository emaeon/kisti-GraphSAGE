{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphSAGE\n",
    "- framework : PyG\n",
    "- dataset : PPI (protein-protein interaction)\n",
    "- node : 21,557\n",
    "- edge : 345,353\n",
    "- feature dim : 50\n",
    "- class : 121\n",
    "- task : node classifiction(단백질의 역할)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.datasets.ppi.PPI"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch_geometric.data import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0,1,1,2],\n",
    "                           [1,0,2,1]], dtype = torch.long) # (2,4) 크기의 행렬 : 4개의 엣지\n",
    "\n",
    "x = torch.tensor([[-1],[0],[1]], dtype = torch.float) # (3,1) 크기의 행렬 : 3개의 노드 \n",
    "\n",
    "data = Data(x=x, edge_index = edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[44906, 50], edge_index=[2, 1226368], y=[44906, 121])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data(x=train_dataset.x,edge_index=train_dataset.edge_index,y=train_dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPI(20)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "from itertools import product\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    "    extract_zip,\n",
    ")\n",
    "from torch_geometric.utils import remove_self_loops\n",
    "\n",
    "\n",
    "[docs]class PPI(InMemoryDataset):\n",
    "    r\"\"\"The protein-protein interaction networks from the `\"Predicting\n",
    "    Multicellular Function through Multi-layer Tissue Networks\"\n",
    "    <https://arxiv.org/abs/1707.04638>`_ paper, containing positional gene\n",
    "    sets, motif gene sets and immunological signatures as features (50 in\n",
    "    total) and gene ontology sets as labels (121 in total).\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        split (str, optional): If :obj:`\"train\"`, loads the training dataset.\n",
    "            If :obj:`\"val\"`, loads the validation dataset.\n",
    "            If :obj:`\"test\"`, loads the test dataset. (default: :obj:`\"train\"`)\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "        pre_filter (callable, optional): A function that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a boolean\n",
    "            value, indicating whether the data object should be included in the\n",
    "            final dataset. (default: :obj:`None`)\n",
    "        force_reload (bool, optional): Whether to re-process the dataset.\n",
    "            (default: :obj:`False`)\n",
    "\n",
    "    **STATS:**\n",
    "\n",
    "    .. list-table::\n",
    "        :widths: 10 10 10 10 10\n",
    "        :header-rows: 1\n",
    "\n",
    "        * - #graphs\n",
    "          - #nodes\n",
    "          - #edges\n",
    "          - #features\n",
    "          - #tasks\n",
    "        * - 20\n",
    "          - ~2,245.3\n",
    "          - ~61,318.4\n",
    "          - 50\n",
    "          - 121\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://data.dgl.ai/dataset/ppi.zip'\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        split: str = 'train',\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "        pre_filter: Optional[Callable] = None,\n",
    "        force_reload: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        assert split in ['train', 'val', 'test']\n",
    "\n",
    "        super().__init__(root, transform, pre_transform, pre_filter,\n",
    "                         force_reload=force_reload)\n",
    "\n",
    "        if split == 'train':\n",
    "            self.load(self.processed_paths[0])\n",
    "        elif split == 'val':\n",
    "            self.load(self.processed_paths[1])\n",
    "        elif split == 'test':\n",
    "            self.load(self.processed_paths[2])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        splits = ['train', 'valid', 'test']\n",
    "        files = ['feats.npy', 'graph_id.npy', 'graph.json', 'labels.npy']\n",
    "        return [f'{split}_{name}' for split, name in product(splits, files)]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> List[str]:\n",
    "        return ['train.pt', 'val.pt', 'test.pt']\n",
    "\n",
    "    def download(self) -> None:\n",
    "        path = download_url(self.url, self.root)\n",
    "        extract_zip(path, self.raw_dir)\n",
    "        os.unlink(path)\n",
    "\n",
    "    def process(self) -> None:\n",
    "        import networkx as nx\n",
    "        from networkx.readwrite import json_graph\n",
    "\n",
    "        for s, split in enumerate(['train', 'valid', 'test']):\n",
    "            path = osp.join(self.raw_dir, f'{split}_graph.json')\n",
    "            with open(path, 'r') as f:\n",
    "                G = nx.DiGraph(json_graph.node_link_graph(json.load(f)))\n",
    "\n",
    "            x = np.load(osp.join(self.raw_dir, f'{split}_feats.npy'))\n",
    "            x = torch.from_numpy(x).to(torch.float)\n",
    "\n",
    "            y = np.load(osp.join(self.raw_dir, f'{split}_labels.npy'))\n",
    "            y = torch.from_numpy(y).to(torch.float)\n",
    "\n",
    "            data_list = []\n",
    "            path = osp.join(self.raw_dir, f'{split}_graph_id.npy')\n",
    "            idx = torch.from_numpy(np.load(path)).to(torch.long)\n",
    "            idx = idx - idx.min()\n",
    "\n",
    "            for i in range(int(idx.max()) + 1):\n",
    "                mask = idx == i\n",
    "\n",
    "                G_s = G.subgraph(\n",
    "                    mask.nonzero(as_tuple=False).view(-1).tolist())\n",
    "                edge_index = torch.tensor(list(G_s.edges)).t().contiguous()\n",
    "                edge_index = edge_index - edge_index.min()\n",
    "                edge_index, _ = remove_self_loops(edge_index)\n",
    "\n",
    "                data = Data(edge_index=edge_index, x=x[mask], y=y[mask])\n",
    "\n",
    "                if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                    continue\n",
    "\n",
    "                if self.pre_transform is not None:\n",
    "                    data = self.pre_transform(data)\n",
    "\n",
    "                data_list.append(data)\n",
    "            self.save(data_list, self.processed_paths[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import PPI\n",
    " \n",
    " \n",
    "# 데이터 로딩\n",
    "train_dataset = PPI(root=\".\", split='train')\n",
    "val_dataset = PPI(root=\".\", split='val')\n",
    "test_dataset = PPI(root=\".\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnumpy\u001b[49m(train_dataset\u001b[38;5;241m.\u001b[39my)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    " \n",
    "# 훈련세트에 이웃 샘플링 적용\n",
    "train_data = Batch.from_data_list(train_dataset)\n",
    "loader = NeighborLoader(train_data, batch_size=2048, shuffle=True, num_neighbors=[20, 10], num_workers=2, persistent_workers=True)\n",
    " \n",
    "# 데이터 로더\n",
    "train_loader = DataLoader(train_dataset, batch_size=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, y):\n",
    "    \"\"\"정확도 계산\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "    \n",
    "import torch\n",
    "torch.manual_seed(-1)\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    " \n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    \"\"\"GraphSAGE\"\"\"\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.sage1 = SAGEConv(dim_in, dim_h) # default = mean aggregator\n",
    "        self.sage2 = SAGEConv(dim_h, dim_out)\n",
    " \n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.sage1(x, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.sage2(h, edge_index)\n",
    "        return F.log_softmax(h, dim=1)\n",
    " \n",
    "    def fit(self, data, epochs):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.nn import GraphSAGE\n",
    " \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    "model = GraphSAGE(\n",
    "    in_channels=train_dataset.num_features,\n",
    "    hidden_channels=512,\n",
    "    num_layers=2,\n",
    "    out_channels=train_dataset.num_classes,\n",
    ").to(device)\n",
    " \n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    " \n",
    "def fit():\n",
    "    model.train()\n",
    " \n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out, data.y)\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    " \n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    " \n",
    "    data = next(iter(loader))\n",
    "    out = model(data.x.to(device), data.edge_index.to(device))\n",
    "    preds = (out > 0).float().cpu()\n",
    " \n",
    "    y, pred = data.y.numpy(), preds.numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 0.589 | Val F1-score: 0.3809\n",
      "Epoch  50 | Train Loss: 0.191 | Val F1-score: 0.8431\n",
      "Epoch 100 | Train Loss: 0.144 | Val F1-score: 0.8800\n",
      "Epoch 150 | Train Loss: 0.122 | Val F1-score: 0.8953\n",
      "Epoch 200 | Train Loss: 0.107 | Val F1-score: 0.9046\n",
      "Epoch 250 | Train Loss: 0.099 | Val F1-score: 0.9093\n",
      "Epoch 300 | Train Loss: 0.094 | Val F1-score: 0.9118\n",
      "Test F1-score: 0.9327\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(301):\n",
    "    loss = fit()\n",
    "    val_f1 = test(val_loader)\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Val F1-score: {val_f1:.4f}')\n",
    " \n",
    "print(f'Test F1-score: {test(test_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
